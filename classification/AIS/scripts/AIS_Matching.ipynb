{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIS-Matching Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get granules info from pyraws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See pyraws lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get BBox centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_to_data_dict_day1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First datetime: 2019-04-15 10:35:05\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 'day8'\n",
    "\n",
    "# day1 = 2019-02-15\n",
    "# day2 = 2019-01-21\n",
    "# day3 = 2019-07-24\n",
    "# day4 = 2019-07-27\n",
    "# day5 = 2019-04-20\n",
    "# day6 = 2019-04-21\n",
    "# day7 = 2019-06-24\n",
    "# day8 = 2019-04-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary created with pyraws\n",
    "with open(f'/Data_large/marine/Datasets/V2RAW/B_02_03_04_08_update/granules/quasiupdate_image_data_{day}.pkl', 'rb') as f:\n",
    "    image_to_data_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load the dictionary that contains the BBoxes' center\n",
    "with open(f'/Data_large/marine/Datasets/V2RAW/B_02_03_04_08_update/dict/{day}_coords_georeferenced.pkl', 'rb') as f:\n",
    "#with open(f'/Data_large/marine/PythonProjects/db/VENuS/annotations/{day}_coords_georeferenced.pkl', 'rb') as f:\n",
    "    bbox_centers_dict = pickle.load(f)\n",
    "    \n",
    "# Get the first key from the dictionary to set the AIS data\n",
    "first_key = next(iter(image_to_data_dict))\n",
    "\n",
    "# Extract the first datetime from the first key's data\n",
    "first_datetime = image_to_data_dict[first_key][1][0]\n",
    "\n",
    "# Print the result\n",
    "print(\"First datetime:\", first_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load AIS data\n",
    "#df = pd.read_csv('/Data_large/marine/Datasets/DATA_RAW_VESSEL_S2/aisdk-2019-02/aisdk-2019-02-15.csv')\n",
    "df = pd.read_csv('/Data_large/marine/Datasets/DATA_RAW_VESSEL_S2/aisdk/aisdk_zip/2019-04/aisdk-2019-04-15.csv')\n",
    "df['Timestamp'] = pd.to_datetime(df['# Timestamp'], dayfirst=True)\n",
    "df['Time'] = df['Timestamp'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_longitude(lon):\n",
    "    if lon >= 179 or lon <= -179:\n",
    "        lon = lon + 180 if lon < 0 else lon - 180\n",
    "    return lon\n",
    "\n",
    "def euclidean_distance(lat1, lon1, lat2, lon2):\n",
    "    return np.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2)\n",
    "\n",
    "def perpendicular_distance(line, point):\n",
    "    return line.distance(point)\n",
    "\n",
    "def navigational_status_weight(status):\n",
    "    if status == 'Engaged in fishing':\n",
    "        return 0.5  # Prioritize fishing ships by reducing cost\n",
    "    return 1.0  # Default weight\n",
    "\n",
    "def process_single_image(selected_key, image_dict, df, bbox_centers_dict):\n",
    "    matched_entries = []\n",
    "    unmatched_entries = []\n",
    "    bbox_gdfs = None\n",
    "\n",
    "    entry = image_dict[selected_key]\n",
    "    \n",
    "    # 1. Extract sensing time and adjust it\n",
    "    sensing_time = entry[1][0] + pd.Timedelta(seconds=1.8)\n",
    "    start_time = (sensing_time - pd.Timedelta(seconds=15)).time()\n",
    "    end_time = (sensing_time + pd.Timedelta(seconds=15)).time()\n",
    "\n",
    "    # 2. Time filtering: Filter AIS data within the time window\n",
    "    df_time_filtered = df[(df['Time'] >= start_time) & (df['Time'] <= end_time)].copy()\n",
    "\n",
    "    # 3. Extract and create a polygon from the image coordinates, reversing them and normalizing longitudes\n",
    "    coordinates = entry[6]\n",
    "    reversed_coordinates = [(normalize_longitude(lon), lat) for lat, lon in coordinates]\n",
    "    polygon = Polygon(reversed_coordinates)\n",
    "\n",
    "    # 4. Spatial filtering: Filter AIS data within the polygon\n",
    "    df_time_filtered['geometry'] = [Point(normalize_longitude(lon), lat) for lon, lat in zip(df_time_filtered['Longitude'], df_time_filtered['Latitude'])]\n",
    "    geo_df = gpd.GeoDataFrame(df_time_filtered, geometry='geometry')\n",
    "    inside_polygon = geo_df.within(polygon)\n",
    "    geo_df_within_polygon = geo_df.loc[inside_polygon].copy()\n",
    "\n",
    "    if not geo_df_within_polygon.empty:\n",
    "        mmsi_to_coords = {}\n",
    "\n",
    "        # Process each unique MMSI: Get the nearest two timestamps for each MMSI\n",
    "        for mmsi in geo_df_within_polygon['MMSI'].unique():\n",
    "            mmsi_filtered = geo_df_within_polygon.loc[geo_df_within_polygon['MMSI'] == mmsi].copy()\n",
    "            mmsi_filtered['time_diff'] = (mmsi_filtered['Timestamp'] - sensing_time).abs()\n",
    "            sorted_entries = mmsi_filtered.sort_values('time_diff').iloc[:2]\n",
    "            coordinates_list = sorted_entries[['Latitude', 'Longitude']].values.tolist()\n",
    "            heading = sorted_entries.iloc[0]['Heading']\n",
    "            navigational_status = sorted_entries.iloc[0]['Navigational status']\n",
    "            mmsi_to_coords[mmsi] = {\n",
    "                'coords': coordinates_list,\n",
    "                'heading': heading,\n",
    "                'navigational_status': navigational_status,\n",
    "                'info': sorted_entries[['Timestamp', 'Heading', 'Navigational status', 'Name', 'Ship type', 'Cargo type']].to_dict(orient='records')\n",
    "            }\n",
    "\n",
    "        # Matching algorithm with Hungarian Algorithm\n",
    "        if selected_key in bbox_centers_dict:\n",
    "            bbox_centers = bbox_centers_dict[selected_key]\n",
    "            used_mmsi = set()\n",
    "            bbox_centers_list = []  # List to store bbox centers for each image\n",
    "\n",
    "            bbox_points = [Point(normalize_longitude(center[1]), center[0]) for _, _, _, center in bbox_centers]\n",
    "\n",
    "            cost_matrix = np.zeros((len(bbox_points), len(mmsi_to_coords)))\n",
    "\n",
    "            for i, bbox_point in enumerate(bbox_points):\n",
    "                for j, (mmsi, data) in enumerate(mmsi_to_coords.items()):\n",
    "                    coords = data['coords']\n",
    "                    heading = data['heading']\n",
    "                    navigational_status = data['navigational_status']\n",
    "                    \n",
    "                    if len(coords) == 2:\n",
    "                        line = LineString([(normalize_longitude(coords[0][1]), coords[0][0]), (normalize_longitude(coords[1][1]), coords[1][0])])\n",
    "                        perp_distance = perpendicular_distance(line, bbox_point)\n",
    "                    else:\n",
    "                        perp_distance = euclidean_distance(bbox_point.y, bbox_point.x, coords[0][0], normalize_longitude(coords[0][1]))\n",
    "                    \n",
    "                    eucl_distance = euclidean_distance(bbox_point.y, bbox_point.x, coords[0][0], normalize_longitude(coords[0][1]))\n",
    "                    status_weight = navigational_status_weight(navigational_status)\n",
    "                    \n",
    "                    cost_matrix[i, j] = status_weight * (perp_distance + eucl_distance)\n",
    "\n",
    "            # Handle invalid numeric entries in the cost matrix\n",
    "            cost_matrix[np.isnan(cost_matrix)] = float('inf')\n",
    "            cost_matrix[np.isinf(cost_matrix)] = float('inf')\n",
    "\n",
    "            if np.all(cost_matrix == float('inf')) or cost_matrix.size == 0:\n",
    "                print(f\"No feasible matches for {selected_key}. All distances are infinite or cost matrix is empty.\")\n",
    "                unmatched_entries = [{'bbox_center': bbox_centers[i][3]} for i in range(len(bbox_centers))]\n",
    "            else:\n",
    "                try:\n",
    "                    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "                except ValueError as e:\n",
    "                    print(f\"ValueError for {selected_key}: {e}\")\n",
    "                    unmatched_entries = [{'bbox_center': bbox_centers[i][3]} for i in range(len(bbox_centers))]\n",
    "                    row_ind, col_ind = [], []\n",
    "\n",
    "                for i, bbox_index in enumerate(row_ind):\n",
    "                    mmsi_index = col_ind[i]\n",
    "                    if cost_matrix[bbox_index, mmsi_index] != float('inf'):\n",
    "                        mmsi = list(mmsi_to_coords.keys())[mmsi_index]\n",
    "                        data = mmsi_to_coords[mmsi]\n",
    "                        matched_entries.append({\n",
    "                            'mmsi': mmsi,\n",
    "                            'coordinates': [data['coords'][0]],  # Use only the nearest AIS coordinate\n",
    "                            'bbox_center': bbox_centers[bbox_index][3],\n",
    "                            'distance': cost_matrix[bbox_index, mmsi_index],\n",
    "                            'info': data['info']\n",
    "                        })\n",
    "                        used_mmsi.add(mmsi)\n",
    "\n",
    "                # Add unmatched MMSIs\n",
    "                for mmsi, data in mmsi_to_coords.items():\n",
    "                    if mmsi not in used_mmsi:\n",
    "                        unmatched_entries.append({\n",
    "                            'mmsi': mmsi,\n",
    "                            'coordinates': data['coords'],\n",
    "                            'info': data['info']\n",
    "                        })\n",
    "\n",
    "                # Store the bbox centers as a list of dictionaries\n",
    "                bbox_centers_list = [{'geometry': Point(normalize_longitude(center[1]), center[0]), 'lat': center[0], 'lon': center[1]} for _, _, _, center in bbox_centers]\n",
    "                \n",
    "                # Convert list to GeoDataFrame\n",
    "                bbox_gdfs = gpd.GeoDataFrame(bbox_centers_list)\n",
    "\n",
    "    # Save the AIS GeoDataFrame after time and spatial filtering:\n",
    "        # .shp: The main file that contains the geometric data (points, lines, polygons).\n",
    "        # .shx: The shape index file, which stores the index of the geometry.\n",
    "        # .dbf: The attribute data file, which stores the attribute data for each shape in a tabular format.\n",
    "        # .prj: The projection file, which contains information about the coordinate system and projection of the geometric data.\n",
    "    if not geo_df_within_polygon.empty:\n",
    "        geo_df_within_polygon = geo_df_within_polygon.copy()\n",
    "        geo_df_within_polygon['Timestamp'] = geo_df_within_polygon['Timestamp'].astype(str)  # Convert datetime to string\n",
    "        geo_df_within_polygon['Time'] = geo_df_within_polygon['Time'].astype(str)  # Convert time to string\n",
    "        \n",
    "        #ais_gdf_dir = '/Data_large/marine/prova/VENuS_S2/S2/AIS/results2/gdf'\n",
    "        ais_gdf_dir ='/Data_large/marine/Datasets/V2RAW/B_02_03_04_08_update/prova/gdf'\n",
    "        # Construct the output directory path\n",
    "        ais_gdf_output_dir = os.path.join(ais_gdf_dir, selected_key)\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(ais_gdf_output_dir, exist_ok=True)\n",
    "        \n",
    "        output_file = os.path.join(ais_gdf_output_dir, f'AIS_filtered_{selected_key}.shp')\n",
    "        geo_df_within_polygon.to_file(output_file)\n",
    "        \n",
    "        #geo_df_within_polygon.to_file(f'/Data_large/marine/PythonProjects/db/VENuS/results/S2_AIS_matching/gdf/{selected_key}/AIS_filtered_{selected_key}.shp')\n",
    "\n",
    "    return matched_entries, unmatched_entries, bbox_gdfs\n",
    "\n",
    "def process_all_images(image_dict, df, bbox_centers_dict):\n",
    "    all_matched_entries = {}\n",
    "    all_unmatched_entries = {}\n",
    "    all_bbox_gdfs = {}\n",
    "\n",
    "    for selected_key in image_dict.keys():\n",
    "        matched_entries, unmatched_entries, bbox_gdfs = process_single_image(selected_key, image_dict, df, bbox_centers_dict)\n",
    "        all_matched_entries[selected_key] = matched_entries\n",
    "        all_unmatched_entries[selected_key] = unmatched_entries\n",
    "        all_bbox_gdfs[selected_key] = bbox_gdfs\n",
    "\n",
    "    return all_matched_entries, all_unmatched_entries, all_bbox_gdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict = image_to_data_dict\n",
    "\n",
    "# ## SINGLE IMAGE PROCESS\n",
    "# selected_key = 'day1_g_21_coreg.tif'\n",
    "# matched_entries, unmatched_entries, bbox_gdfs = process_single_image(selected_key, image_dict, df, bbox_centers_dict)\n",
    "\n",
    "# ALL IMAGES PROCESS\n",
    "all_matched_data, all_unmatched_data, all_bbox_gdfs = process_all_images(image_to_data_dict, df, bbox_centers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the matched data to a pickle file for further analysis\n",
    "with open(f'/Data_large/marine/Datasets/V2RAW/B_02_03_04_08_update/prova/results/{day}_matched_hung.pkl', 'wb') as f:\n",
    "    pickle.dump(all_matched_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Read and check\n",
    "with open(f'/Data_large/marine/Datasets/V2RAW/B_02_03_04_08_update/prova/results/day8_matched_hung.pkl', 'rb') as f:\n",
    "    match_read=pickle.load(f)\n",
    "    \n",
    "match_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the matched data to a pickle file for further analysis\n",
    "with open(f'/Data_large/marine/prova/VENuS_S2/S2/AIS/results2/{day}_matched_hung.pkl', 'wb') as f:\n",
    "    pickle.dump(all_matched_data, f)\n",
    "\n",
    "# Save the unmatched data to a pickle file for further analysis\n",
    "with open(f'/Data_large/marine/prova/VENuS_S2/S2/AIS/results2/{day}_unmatched_hung.pkl', 'wb') as f:\n",
    "    pickle.dump(all_unmatched_data, f)\n",
    "\n",
    "# Optionally, save bbox GeoDataFrames to shapefiles\n",
    "for key, gdf in all_bbox_gdfs.items():\n",
    "    if gdf is not None and not gdf.empty:        \n",
    "        gdf.to_file(f'/Data_large/marine/prova/VENuS_S2/S2/AIS/results2/bbox/bbox_centers_{key}.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
